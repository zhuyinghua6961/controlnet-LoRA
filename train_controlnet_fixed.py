import os
import yaml
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from accelerate import Accelerator
from diffusers import ControlNetModel, AutoencoderKL, UNet2DConditionModel, DDPMScheduler
from transformers import CLIPTextModel, CLIPTokenizer
from diffusers.optimization import get_scheduler
from tqdm.auto import tqdm
from dataset import RadarControlNetDataset
import argparse
import time
import glob

def load_config(path):
    with open(path, 'r') as f:
        return yaml.safe_load(f)

def find_latest_checkpoint(output_dir):
    checkpoints = glob.glob(os.path.join(output_dir, "checkpoint-*"))
    if not checkpoints:
        return None
    checkpoints = sorted(checkpoints, key=lambda x: int(x.split("-")[-1]))
    return checkpoints[-1] if checkpoints else None

def compute_improved_weighted_loss(model_pred, noise, conditioning_heatmap, weight_factor=50.0, use_focal=False):
    """
    æ”¹è¿›çš„åŠ æƒLosså‡½æ•° - ä¿®å¤3ä¸ªå…³é”®é—®é¢˜
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. ä½¿ç”¨Max Poolingä¸‹é‡‡æ ·ï¼ˆä¿æŒå³°å€¼å¼ºåº¦ï¼‰
    2. äºŒå€¼åŒ–æƒé‡ï¼ˆç›®æ ‡åŒºåŸŸæé«˜æƒé‡ï¼ŒèƒŒæ™¯æ ‡å‡†æƒé‡ï¼‰
    3. å¯é€‰ï¼šFocal Lossæœºåˆ¶ï¼ˆå…³æ³¨éš¾å­¦ä¹ çš„åŒºåŸŸï¼‰
    
    Args:
        model_pred: UNeté¢„æµ‹çš„å™ªå£° [B, 4, 64, 64]
        noise: çœŸå®å™ªå£° [B, 4, 64, 64]
        conditioning_heatmap: çƒ­åŠ›å›¾æ¡ä»¶ [B, 3, 512, 512]
        weight_factor: ç›®æ ‡åŒºåŸŸçš„æƒé‡å€æ•°
        use_focal: æ˜¯å¦ä½¿ç”¨Focal Lossæœºåˆ¶
    
    Returns:
        weighted_loss: åŠ æƒåçš„æŸå¤±
        stats: ç»Ÿè®¡ä¿¡æ¯
    """
    # 1. åŸºç¡€MSEæŸå¤±ï¼ˆé€åƒç´ ï¼‰
    base_loss = F.mse_loss(model_pred.float(), noise.float(), reduction="none")  # [B, 4, 64, 64]
    
    # 2. ç”Ÿæˆæƒé‡å›¾ - ä½¿ç”¨Max Poolingä¿æŒå³°å€¼
    heatmap_gray = conditioning_heatmap.mean(dim=1, keepdim=True)  # [B, 1, 512, 512]
    
    # ğŸ”´ å…³é”®æ”¹è¿›1ï¼šä½¿ç”¨Max Poolingä»£æ›¿Bilinearï¼ˆä¿æŒå³°å€¼å¼ºåº¦ï¼‰
    # 512 -> 64 éœ€è¦ä¸‹é‡‡æ ·8å€ï¼Œä½¿ç”¨kernel_size=8, stride=8
    weight_map = F.max_pool2d(heatmap_gray, kernel_size=8, stride=8)  # [B, 1, 64, 64]
    
    # ğŸ”´ å…³é”®æ”¹è¿›2ï¼šäºŒå€¼åŒ–æƒé‡ï¼ˆç›®æ ‡/èƒŒæ™¯æ˜ç¡®åŒºåˆ†ï¼‰
    # é˜ˆå€¼é€‰æ‹©ï¼šçƒ­å›¾å€¼>0.1è®¤ä¸ºæ˜¯ç›®æ ‡åŒºåŸŸ
    target_mask = (weight_map > 0.1).float()  # [B, 1, 64, 64]
    
    # ç›®æ ‡åŒºåŸŸï¼šweight_factorå€æƒé‡ï¼›èƒŒæ™¯åŒºåŸŸï¼š1å€æƒé‡
    weight_map = torch.where(
        target_mask > 0.5,
        torch.full_like(weight_map, float(weight_factor)),  # ç›®æ ‡åŒºåŸŸ
        torch.ones_like(weight_map)  # èƒŒæ™¯åŒºåŸŸ
    )
    
    # ğŸ”´ å…³é”®æ”¹è¿›3ï¼ˆå¯é€‰ï¼‰ï¼šFocal Loss - ç»™éš¾å­¦ä¹ çš„æ ·æœ¬æ›´é«˜æƒé‡
    if use_focal:
        # Focal loss: ç»™é«˜lossçš„åŒºåŸŸæ›´é«˜æƒé‡ï¼ˆæ¨¡å‹è¿˜æ²¡å­¦å¥½çš„åœ°æ–¹ï¼‰
        focal_weight = torch.pow(base_loss.detach() / base_loss.detach().mean(), 0.5)
        weight_map = weight_map * (0.5 + 0.5 * focal_weight.mean(dim=1, keepdim=True))
    
    # 4. åº”ç”¨æƒé‡ï¼ˆæ‰©å±•åˆ°4ä¸ªé€šé“ï¼‰
    weight_map = weight_map.expand_as(base_loss)  # [B, 4, 64, 64]
    weighted_loss = (base_loss * weight_map).sum() / weight_map.sum()  # åŠ æƒå¹³å‡
    
    # 5. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºç›‘æ§ï¼‰
    with torch.no_grad():
        base_loss_mean = base_loss.mean().item()
        weight_mean = weight_map.mean().item()
        target_ratio = target_mask.mean().item()  # ç›®æ ‡åŒºåŸŸå æ¯”
        effective_weight = weight_mean  # å®é™…å¹³å‡æƒé‡
        
        # è®¡ç®—ç›®æ ‡åŒºåŸŸå’ŒèƒŒæ™¯åŒºåŸŸçš„loss
        target_loss = (base_loss * target_mask.expand_as(base_loss)).sum() / (target_mask.sum() * 4 + 1e-6)
        bg_loss = (base_loss * (1 - target_mask.expand_as(base_loss))).sum() / ((1 - target_mask).sum() * 4 + 1e-6)
    
    return weighted_loss, {
        'base_loss': base_loss_mean,
        'weight_mean': effective_weight,
        'target_ratio': target_ratio,
        'target_loss': target_loss.item(),
        'bg_loss': bg_loss.item(),
    }

def main(config_path, resume_from_checkpoint=None):
    config = load_config(config_path)
    proj = config["project"]
    data_cfg = config["data"]
    train_cfg = config["training"]
    
    # ä»é…ç½®ä¸­è¯»å–å‚æ•°
    weight_factor = train_cfg.get("loss_weight_factor", 50.0)
    use_focal_loss = train_cfg.get("use_focal_loss", False)
    print(f"ğŸ¯ ä½¿ç”¨æ”¹è¿›çš„åŠ æƒLoss")
    print(f"   - æƒé‡å› å­: {weight_factor}x")
    print(f"   - Max Poolingä¸‹é‡‡æ ·ï¼ˆä¿æŒå³°å€¼ï¼‰")
    print(f"   - äºŒå€¼åŒ–æƒé‡ï¼ˆç›®æ ‡{weight_factor}xï¼ŒèƒŒæ™¯1xï¼‰")
    print(f"   - Focal Loss: {'å¯ç”¨' if use_focal_loss else 'ç¦ç”¨'}")

    accelerator = Accelerator(
        gradient_accumulation_steps=train_cfg["gradient_accumulation_steps"],
        mixed_precision=train_cfg["mixed_precision"],
    )
    
    # åˆå§‹åŒ–TensorBoard
    writer = None
    if accelerator.is_main_process:
        try:
            tensorboard_dir = os.path.join(proj["output_dir"], "logs")
            os.makedirs(tensorboard_dir, exist_ok=True)
            writer = SummaryWriter(log_dir=tensorboard_dir)
        except Exception:
            writer = None

    print("ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹...")
    tokenizer = CLIPTokenizer.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="unet")
    controlnet = ControlNetModel.from_unet(unet)
    noise_scheduler = DDPMScheduler.from_pretrained("runwayml/stable-diffusion-v1-5", subfolder="scheduler")

    # å¯ç”¨xformersï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        import xformers
        unet.enable_xformers_memory_efficient_attention()
        controlnet.enable_xformers_memory_efficient_attention()
        print("âœ… å¯ç”¨ xformers å†…å­˜ä¼˜åŒ–")
    except ImportError:
        print("âš ï¸  xformers æœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶")

    vae = vae.to(accelerator.device)
    text_encoder = text_encoder.to(accelerator.device)
    unet = unet.to(accelerator.device)

    vae.requires_grad_(False)
    unet.requires_grad_(False)
    text_encoder.requires_grad_(False)
    controlnet.train()

    # å…¼å®¹å­—ç¬¦ä¸²æˆ–æ•°å­—å½¢å¼çš„å­¦ä¹ ç‡é…ç½®ï¼ˆä¾‹å¦‚ "1e-5" æˆ– 1e-5ï¼‰
    lr_raw = train_cfg.get("learning_rate", train_cfg.get("lr", 1e-5))
    try:
        learning_rate = float(lr_raw)
    except Exception:
        raise ValueError(f"training.learning_rate å¿…é¡»æ˜¯æ•°å€¼ï¼Œå½“å‰ä¸º: {lr_raw!r}")

    optimizer = torch.optim.AdamW(controlnet.parameters(), lr=learning_rate)

    # ä»é…ç½®è¯»å–sigmaå‚æ•°
    heatmap_sigma = train_cfg.get("heatmap_sigma", 15.0)
    print(f"ğŸ¯ çƒ­åŠ›å›¾Sigma: {heatmap_sigma}")
    
    dataset = RadarControlNetDataset(
        data_dir=data_cfg["data_dir"],
        tokenizer=tokenizer,
        size=data_cfg["resolution"],
        range_max=data_cfg["range_max"],
        vel_max=data_cfg["vel_max"],
        heatmap_sigma=heatmap_sigma
    )
    train_dataloader = DataLoader(
        dataset,
        batch_size=train_cfg["train_batch_size"],
        shuffle=True,
        num_workers=train_cfg["dataloader_num_workers"]
    )

    starting_epoch = 0
    global_step = 0
    resume_step = 0

    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = len(train_dataloader) // train_cfg["gradient_accumulation_steps"]
    max_train_steps = train_cfg["num_train_epochs"] * num_update_steps_per_epoch

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    print(f"ğŸ“Š å­¦ä¹ ç‡è°ƒåº¦å™¨: {train_cfg['lr_scheduler']}")
    lr_scheduler = get_scheduler(
        train_cfg["lr_scheduler"],
        optimizer=optimizer,
        num_warmup_steps=train_cfg["lr_warmup_steps"] * train_cfg["gradient_accumulation_steps"],
        num_training_steps=max_train_steps * train_cfg["gradient_accumulation_steps"],
    )

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤è®­ç»ƒ
    if resume_from_checkpoint == "none":
        print("ğŸ†• å¼ºåˆ¶ä»å¤´å¼€å§‹è®­ç»ƒï¼ˆ--from_scratchï¼‰")
        resume_from_checkpoint = None
    elif resume_from_checkpoint is None:
        resume_from_checkpoint = find_latest_checkpoint(proj["output_dir"])
        if resume_from_checkpoint:
            print(f"ğŸ” è‡ªåŠ¨å‘ç°æœ€æ–° checkpoint: {resume_from_checkpoint}")
    else:
        print(f"ğŸ“Œ ä½¿ç”¨æŒ‡å®š checkpoint: {resume_from_checkpoint}")

    # Prepareæ‰€æœ‰ç»„ä»¶
    print("ğŸ”§ Preparing models with Accelerator...")
    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        controlnet, optimizer, train_dataloader, lr_scheduler
    )

    # å¦‚æœæœ‰checkpointï¼ŒåŠ è½½æ¨¡å‹æƒé‡å’Œè®­ç»ƒçŠ¶æ€
    if resume_from_checkpoint:
        print(f"ğŸ”„ ä» checkpoint æ¢å¤: {resume_from_checkpoint}")
        
        controlnet_state = ControlNetModel.from_pretrained(resume_from_checkpoint)
        accelerator.unwrap_model(controlnet).load_state_dict(controlnet_state.state_dict())
        
        trainer_state_path = os.path.join(resume_from_checkpoint, "trainer_state.yaml")
        if os.path.exists(trainer_state_path):
            with open(trainer_state_path, 'r') as f:
                trainer_state = yaml.safe_load(f)
            starting_epoch = trainer_state.get("epoch", 0)
            global_step = trainer_state.get("global_step", 0)
            resume_step = trainer_state.get("resume_step", 0)
            
            if resume_step == 0:
                starting_epoch += 1
                print(f"âœ… æ¢å¤çŠ¶æ€: å·²å®Œæˆ {starting_epoch} ä¸ªepochï¼Œä» Epoch {starting_epoch + 1} ç»§ç»­è®­ç»ƒ")
            else:
                print(f"âœ… æ¢å¤çŠ¶æ€: ä» Epoch {starting_epoch + 1} ç¬¬ {resume_step} æ­¥ç»§ç»­è®­ç»ƒ")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒçŠ¶æ€æ–‡ä»¶ï¼Œä» Epoch 1 å¼€å§‹")
    else:
        print("ğŸ†• ä»å¤´å¼€å§‹è®­ç»ƒ")

    num_update_steps_per_epoch = len(train_dataloader) // train_cfg["gradient_accumulation_steps"]

    total_start_time = time.time()
    
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ")
    print("="*60)
    
    for epoch in range(starting_epoch, train_cfg["num_train_epochs"]):
        epoch_start_time = time.time()
        epoch_weighted_loss = 0.0
        epoch_base_loss = 0.0
        epoch_target_loss = 0.0
        epoch_bg_loss = 0.0
        num_batches = 0
        
        if resume_step > 0:
            train_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)
            resume_step = 0

        progress_bar = tqdm(
            total=num_update_steps_per_epoch,
            desc=f"Epoch {epoch + 1}/{train_cfg['num_train_epochs']}",
            disable=not accelerator.is_local_main_process
        )
        
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(controlnet):
                latents = vae.encode(batch["pixel_values"].to(dtype=vae.dtype)).latent_dist.sample()
                latents = latents * vae.config.scaling_factor

                noise = torch.randn_like(latents)
                bsz = latents.shape[0]
                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                encoder_hidden_states = text_encoder(batch["input_ids"])[0]

                down_block_res_samples, mid_block_res_sample = controlnet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=encoder_hidden_states,
                    controlnet_cond=batch["conditioning_pixel_values"],
                    return_dict=False,
                )

                model_pred = unet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states=encoder_hidden_states,
                    down_block_additional_residuals=down_block_res_samples,
                    mid_block_additional_residual=mid_block_res_sample,
                ).sample

                # ğŸ”´ ä½¿ç”¨æ”¹è¿›çš„åŠ æƒLosså‡½æ•°
                loss, loss_stats = compute_improved_weighted_loss(
                    model_pred, 
                    noise, 
                    batch["conditioning_pixel_values"],
                    weight_factor=weight_factor,
                    use_focal=use_focal_loss
                )

                accelerator.backward(loss)
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(controlnet.parameters(), train_cfg["max_grad_norm"])
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

                if accelerator.sync_gradients:
                    epoch_weighted_loss += loss.item()
                    epoch_base_loss += loss_stats['base_loss']
                    epoch_target_loss += loss_stats['target_loss']
                    epoch_bg_loss += loss_stats['bg_loss']
                    num_batches += 1
                    global_step += 1
                    
                    avg_weighted_loss = epoch_weighted_loss / num_batches
                    avg_base_loss = epoch_base_loss / num_batches
                    
                    # è®°å½•åˆ°TensorBoard
                    if writer is not None:
                        try:
                            writer.add_scalar('Loss/weighted_loss', loss.item(), global_step)
                            writer.add_scalar('Loss/base_loss', loss_stats['base_loss'], global_step)
                            writer.add_scalar('Loss/target_loss', loss_stats['target_loss'], global_step)
                            writer.add_scalar('Loss/bg_loss', loss_stats['bg_loss'], global_step)
                            writer.add_scalar('Loss/weight_mean', loss_stats['weight_mean'], global_step)
                            writer.add_scalar('Loss/target_ratio', loss_stats['target_ratio'], global_step)
                            writer.add_scalar('Training/learning_rate', optimizer.param_groups[0]['lr'], global_step)
                        except Exception:
                            pass
                    
                    # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
                    progress_bar.set_postfix({
                        "w_loss": f"{avg_weighted_loss:.4f}",
                        "base": f"{avg_base_loss:.4f}",
                        "t_loss": f"{loss_stats['target_loss']:.4f}",
                        "bg": f"{loss_stats['bg_loss']:.4f}"
                    })
                    progress_bar.update(1)
                    
                    # å®šæœŸä¿å­˜checkpoint
                    if global_step % train_cfg["save_steps"] == 0:
                        if accelerator.is_main_process:
                            save_path = os.path.join(proj["output_dir"], f"checkpoint-{global_step}")
                            os.makedirs(save_path, exist_ok=True)
                            accelerator.unwrap_model(controlnet).save_pretrained(save_path)
                            trainer_state = {
                                "epoch": epoch,
                                "global_step": global_step,
                                "resume_step": step + 1
                            }
                            with open(os.path.join(save_path, "trainer_state.yaml"), 'w') as f:
                                yaml.dump(trainer_state, f)

        progress_bar.close()
        
        epoch_time = time.time() - epoch_start_time
        avg_weighted_loss = epoch_weighted_loss / num_batches if num_batches > 0 else 0
        avg_base_loss = epoch_base_loss / num_batches if num_batches > 0 else 0
        avg_target_loss = epoch_target_loss / num_batches if num_batches > 0 else 0
        avg_bg_loss = epoch_bg_loss / num_batches if num_batches > 0 else 0
        
        elapsed_total = time.time() - total_start_time
        epochs_done = epoch + 1 - starting_epoch
        epochs_left = train_cfg["num_train_epochs"] - (epoch + 1)
        avg_epoch_time = elapsed_total / epochs_done if epochs_done > 0 else 0
        total_eta = avg_epoch_time * epochs_left
        
        # è®°å½•epochçº§åˆ«çš„æŒ‡æ ‡
        if writer is not None:
            try:
                writer.add_scalar('Loss/weighted_loss_epoch', avg_weighted_loss, epoch + 1)
                writer.add_scalar('Loss/base_loss_epoch', avg_base_loss, epoch + 1)
                writer.add_scalar('Loss/target_loss_epoch', avg_target_loss, epoch + 1)
                writer.add_scalar('Loss/bg_loss_epoch', avg_bg_loss, epoch + 1)
                writer.add_scalar('Training/epoch_time_minutes', epoch_time / 60, epoch + 1)
                writer.add_scalar('Training/samples_per_second', len(dataset) / epoch_time, epoch + 1)
                writer.add_scalar('Training/learning_rate_epoch', optimizer.param_groups[0]['lr'], epoch + 1)
            except Exception:
                pass
        
        print(f"\nâœ… Epoch {epoch + 1} å®Œæˆ:")
        print(f"   Weighted Loss: {avg_weighted_loss:.4f}")
        print(f"   Base Loss: {avg_base_loss:.4f}")
        print(f"   Target Loss: {avg_target_loss:.4f} | BG Loss: {avg_bg_loss:.4f}")
        print(f"   ç”¨æ—¶: {epoch_time/60:.1f}åˆ†é’Ÿ | æ€»ETA: {total_eta/60:.1f}åˆ†é’Ÿ")
        
        # æ¯5ä¸ªepochä¿å­˜checkpoint
        if accelerator.is_main_process and (epoch + 1) % 5 == 0:
            save_path = os.path.join(proj["output_dir"], f"checkpoint-epoch-{epoch + 1}")
            os.makedirs(save_path, exist_ok=True)
            accelerator.unwrap_model(controlnet).save_pretrained(save_path)
            trainer_state = {
                "epoch": epoch,
                "global_step": global_step,
                "resume_step": 0
            }
            with open(os.path.join(save_path, "trainer_state.yaml"), 'w') as f:
                yaml.dump(trainer_state, f)
            print(f"ğŸ’¾ Checkpoint saved: {save_path}")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if accelerator.is_main_process:
        save_path = os.path.join(proj["output_dir"], "controlnet")
        accelerator.unwrap_model(controlnet).save_pretrained(save_path)
        
        if writer is not None:
            try:
                writer.close()
            except Exception:
                pass
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³: {save_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="é›·è¾¾ RD å›¾ ControlNet è®­ç»ƒï¼ˆæ”¹è¿›Lossç‰ˆæœ¬ï¼‰")
    parser.add_argument("--config", type=str, default="config.yaml",
                        help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--resume_from_checkpoint", type=str, default=None,
                        help="æŒ‡å®š checkpoint è·¯å¾„")
    parser.add_argument("--from_scratch", action="store_true",
                        help="å¼ºåˆ¶ä»å¤´å¼€å§‹è®­ç»ƒ")
    args = parser.parse_args()

    os.makedirs("output", exist_ok=True)
    
    if args.from_scratch:
        print("âš¡ å¼ºåˆ¶ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å¼")
        resume_checkpoint = "none"
    else:
        resume_checkpoint = args.resume_from_checkpoint
    
    main(args.config, resume_checkpoint)


